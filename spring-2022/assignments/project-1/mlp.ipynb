{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "white-scenario",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron for Image Classification\n",
    "\n",
    "Welome to the **first** programming assignment of the Deep Learning course. \n",
    "\n",
    "In this assignment, a complete set of Jupyter Notebook and python scripts are prepared for examining almost all critical factors in designing a Multi-layer Perceptron model.\n",
    "\n",
    "You will complete the python scripts which have been provided in the project directory to build a deep network, and apply it to a multi-label classification problem. Additionally you will play around varoius factors which may enhance the performance of a deep neural network model.\n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    " - Build and apply a deep neural network to supervised learning using PyTorch framework.\n",
    " - See how factors like **Regularization techniques**, **Activation functions**, **Number of layers and units**, **Weights initialization** and **Data augmentation** affect the result of learning process.\n",
    " \n",
    "**Before you start:** Please read the Submission section at the bottom of the notebook carefully. \n",
    " \n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-reducing",
   "metadata": {},
   "source": [
    "# 0. Packages and modules\n",
    "\n",
    "Let's first import all the packages that you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "linear-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SignDigitDataset\n",
    "from utils import *\n",
    "from model import MLP\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "federal-duplicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using cpu!\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"You are using {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-divide",
   "metadata": {},
   "source": [
    "# 1. Dataset\n",
    "\n",
    "Sign languages (also known as signed languages) are languages that use manual communication to convey meaning. This can include simultaneously employing hand gestures, movement, orientation of the fingers, arms or body, and facial expressions to convey a speaker's ideas. [Source.](https://en.wikipedia.org/wiki/Sign_language)\n",
    "\n",
    "The dataset that you'll be using during this assignment is a subset of the sign language digits. It contains six different classes representing the digits from 0 to 5. You can see some of the images below.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/samples.png\">\n",
    "</p>\n",
    "\n",
    "**Problem Statement:** You are given two dataset `train_signs.h5` and `test_signs.h5` containing:\n",
    "\n",
    "    -- a training/test set of images labelled as one of the classes in {0, 1, 2, 3, 4, 5}.\n",
    "    -- each image is of shape (64, 64, 3) where 3 is for the 3 channels (RGB).\n",
    "    -- train a MLP model to classify the test images.\n",
    "    \n",
    "Let's get more familiar with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-noise",
   "metadata": {},
   "source": [
    "---\n",
    "## `dataset.py`:\n",
    "\n",
    "One of the first steps for designing a deep learning model is to get to know the data as best as possible. The better we know the data (shape, data distribution, etc.), the more proper model can be designed.\n",
    "\n",
    "For this we must create the dataset which we want to feed the model. As we are using PyTorch for this assignment, you must know how to create **torch data loaders**.\n",
    "\n",
    "In this question you have to complete some parts of `__getitem__()` in `dataset.py` and the following functions in `utils.py`:\n",
    "\n",
    "- `get_transformations()`: for data transformation and also needed augmentations.\n",
    "- `get_one_hot()`: for converting the labels into a model usable labels.\n",
    "- `visualize_samples()`: to plot some of the images to see some samples.\n",
    "\n",
    "\n",
    "**Note:** Before that, please visit the official documentaion of pytorch for creating custom datasets to learn how we create them for pytorch deep learning models.\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-marijuana",
   "metadata": {},
   "source": [
    "### `get_transformations()`:\n",
    "\n",
    "As we mentioned earlier, the shape of the tensor we have now of images is **(n_samples, 64, 64, 3)**. But if we want to feed it to a MLP model, we must consider some transformations:\n",
    "\n",
    "    -- Flattening them to have the shape of (n_samples, 64*64*3).\n",
    "    -- Normalizing each pixel has to have a value between 0 - 1.(Hopefully this will be handled automatically)\n",
    "    -- Convert the numpy arrays to tensors. (using transforms.ToTensor())\n",
    "    \n",
    "<p align=\"center\">\n",
    "  <img src=\"images/imvectorkiank.png\">\n",
    "</p>    \n",
    "    \n",
    "These were the minimum transformation which must be applied on the input raw images. Later in this assignment we will add some random transformations (a.k.a augmentations) which could affect the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-clone",
   "metadata": {},
   "source": [
    "### `get_one_hot()`:\n",
    "\n",
    "Many time in deep learning you will have a $Y$ ranging from 0 to $C-1$, where $C$ is the number of the classes. If $C$ is for example 4, then you might have the following $y$ vector which you will need to convet like this:\n",
    "    \n",
    "<p align=\"center\">\n",
    "  <img src=\"images/onehot.png\">\n",
    "</p>    \n",
    "    \n",
    "This is called **one hot** encoding, because in the converted representation, exactly one element of each column is \"hot\" (meaning set to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-array",
   "metadata": {},
   "source": [
    "### `visualize_samples()`:\n",
    "\n",
    "At the end of this part, we want to plot some random samples of the dataset. To do this, you have to complete `visualize_samples()` in `utils.py` using [Matplotlib](https://matplotlib.org) funcionalities.\n",
    "\n",
    "After completing that function, run the cell below to observe the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SignDigitDataset(root_dir='data/',\n",
    "                                 h5_name='train_signs.h5',\n",
    "                                 train=True,\n",
    "                                 transform=get_transformations(64))\n",
    "\n",
    "test_dataset = SignDigitDataset(root_dir='data/',\n",
    "                                h5_name='test_signs.h5',\n",
    "                                train=False,\n",
    "                                transform=get_transformations(64))\n",
    "\n",
    "\n",
    "visualize_samples(train_dataset, n_samples=16, cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-pearl",
   "metadata": {},
   "source": [
    "# 2. Architecture of your model\n",
    "\n",
    "Now that you are familiar with the dataset, it is time to build a deep neural network to classify sign digit images. All the modification which you have consider in this part, must be applied in `model.py`. It consits the a Pytorch class named as **MLP**. All the torch model classes must inherit `torch.nn.Module`.\n",
    "\n",
    "For a review on how torch models have to be created, please visit the pytorch official documentaion: https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n",
    "\n",
    "More precisely, in this part, you will complete `MLP.__init__()` and `MLP.forward()` functions:\n",
    "\n",
    "- `MLP.__init__()`: In this function, all the parameters will be initialized. such as number of layers and units per layer. Note that the model you are implementing must be capable to have every custom number of layers, neurons and also activation layers.\n",
    "\n",
    "- `MLP.forward()`: For forward propagation purpose. *(~ 1 line of code)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test the implemented functions (for Debugging purposes)\n",
    "batch, C, H, W = 10, 3, 64, 64\n",
    "zero_tensor = torch.zeros([batch, C, H, W], dtype=torch.float32)\n",
    "zero_tensor = torch.reshape(zero_tensor, (batch, -1))\n",
    "input_size = C*H*W\n",
    "num_classes = 6\n",
    "units = [input_size, 7, 7, num_classes]\n",
    "\n",
    "mlp = MLP(units=units, hidden_layer_activation='relu')\n",
    "print(mlp)\n",
    "out = mlp(zero_tensor)\n",
    "assert out.shape == torch.Size([batch, num_classes]), f'Model output size expected to be {torch.Size([batch, num_classes])}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-assumption",
   "metadata": {},
   "source": [
    "# 3. Tarining the model\n",
    "\n",
    "Its time to train your beautiful model :) All you need is a `train.py` file and you have to follow the steps below in order complete it:\n",
    "\n",
    "1. Create train and test data loaders with respect to some hyper-parameters including batch_size.\n",
    "2. Get an instance of your MLP model.\n",
    "3. Define an appropriate loss function (e.g. cross entropy loss)\n",
    "4. Define an optimizers with proper hyperparameters such as (learning_rate, ...).\n",
    "5. Implement the main loop function with n_epochs iterations which the learning and evaluation process occurred there. In this loop, you have to log the training and test set loss and accuracy to see if your model works correctly or not.\n",
    "\n",
    "6. Save the model weights for laters usages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-amsterdam",
   "metadata": {},
   "source": [
    "## 3.1 Train your first MLP model (30 points)\n",
    "\n",
    "After defining the model we need to trigger training process by using the code partly prepared in `train.py` after filling required lines run the following cell to start training. In this file some writers are defined which are later used for plotting visualizations in tensorboard framework. Summary information defined as scalars (like loss) are saved by this writers in logs folder near existing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of how we run the train.py script\n",
    "!python train.py --n_epochs 100 --hidden_layers 25 12 --print_every 10 --learning_rate 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-sheffield",
   "metadata": {},
   "source": [
    "**Important note:** In this step, your code must work without any bugs and errors and you are free to play around your code to run different traininig procedures to have a saved proper model.\n",
    "\n",
    "Also by running the cell below, you can your training curves (e.g. loss and accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the learning curve\n",
    "%load_ext tensorboard \n",
    "%tensorboard --logdir=./runs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-military",
   "metadata": {},
   "source": [
    "# 4. Experimenting on features of an MLP\n",
    "\n",
    "In the rest of this notebook, some experiments should be done on different setting like regularization, activation function, number of layers, etc combined with some visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-blowing",
   "metadata": {},
   "source": [
    "## 4.1 Different initializations (20 points)\n",
    "\n",
    "First we want to see the effect of two common initializers:\n",
    "\n",
    " 1. Zero constant\n",
    " 2. Uniform distribution\n",
    " \n",
    "At first we use zero initializer **both for biases and weights**. By doing so, specially for weight initialization, the network will get in trouble with **breaking the symmetry.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "varied-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 1 STARTS HERE ##############\n",
    "\n",
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 1 ENDS HERE ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-albania",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "\n",
    "Use tensorboard visualizations of learning curves (like loss and accuracy to discuss about the issue raised by using zeros initializer for weights. Does uniform random initializing a good choice as weight initialization method?\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-setting",
   "metadata": {},
   "source": [
    "## 4.2 Different activation functions (15 points)\n",
    "\n",
    "Now, lets try different activation functions and evaluate the existence of activation functions amond hidden layers as a powerful to learn non-linear functions. Hence you have to train you model:\n",
    "\n",
    "1. Without any activation layer (except softmax function for last layer).\n",
    "2. With `nn.Sigmoid()` as hidden layer activation functions.\n",
    "3. With `nn.ReLu()` as hidden layer activation functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 2 STARTS HERE ##############\n",
    "\n",
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 2 ENDS HERE ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-mystery",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "\n",
    "Use tensorboard visualizations of learning curves (like loss and accuracy to discuss about the impact of learning procedures with/without different activation functions for hidden layers. Which one do you chosse as a best case?\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-porcelain",
   "metadata": {},
   "source": [
    "## 4.3 Differnet architectures (20 points)\n",
    "\n",
    "Next, we will cast light on the importance of architecture of neural networks (more specifically number of layers and units).\n",
    "\n",
    "In the first one we have 3 hidden layers with 500, 100, 50 units, respectively. This model imposes high computational cost. On the other hand, the second network has one less hidden layer but same number of units in the rest of layers. Totally it has less complexity in terms of number of parameters compared with the first network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_list = [[500, 100, 50], [100, 50]]\n",
    "\n",
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 3 STARTS HERE ##############\n",
    "\n",
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 3 ENDS HERE ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-overview",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "\n",
    "By using tensorboard visualiztions justify the different outcomes of training the above two neural network architectures.\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-navigator",
   "metadata": {},
   "source": [
    "## 4.4 Overfitting prevention (15 points)\n",
    "\n",
    "Another important point to consider is applying regularization or data augmentation methods and being aware of its impact on training and generalization. Its most important rule is to prevent our learning process from overfitting (you may have encountered this phenomenon above!)\n",
    "\n",
    "In this part you have to modify your codes to support **at least one of the following methods**:\n",
    "\n",
    "1. Add some other useful transformation(augmentation) techniques using `torchvision.transforms`. All the codes for this part must be added in `dataset.py` and `utils.py`. (https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "2. Add dropout to your hidden layers. Try dropout rates such as 0.2 and 0.5. (https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 4 STARTS HERE ##############\n",
    "\n",
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 4 ENDS HERE ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-dream",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "\n",
    "By using tensorboard visualiztions justify the different outcomes of training the above two neural network architectures.\n",
    "To show this, your learning cuvres must include both training and test curves.\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-custom",
   "metadata": {},
   "source": [
    "## 4.5 Hyper-parameters tuning (Optional - 10 points)\n",
    "\n",
    "Great! We saw how different factors would affect the performance of a MLP model. Now its time to find the best model by tuning various hyper-parameters we have. In this part you are welcomed to train the best model. More specifically, you have to:\n",
    "\n",
    "1. Try different hyper-parameters (n_hidden_layers, batch_size, learning_rate, optimizer, momentum rate and etc. )\n",
    "\n",
    "2. save the best model as `best_model.pth` and\n",
    "3. Write all the assumed hyper-parameters in `params.yaml` file.\n",
    "4. Use tenorboard to plot the learning curve and report the final loss and accuracy that you get for both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prescription-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 5 STARTS HERE ##############\n",
    "\n",
    "############## YOUR CODES FOR RUNNING EXPERIMENTS OF QUESTION 5 ENDS HERE ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-procurement",
   "metadata": {},
   "source": [
    "# 5. Submission\n",
    "\n",
    "Please read the notes here carefully:\n",
    "\n",
    "1. In addition to completing the code files, please send a report including your answer to these questions as well. Do not forget to put the diagrams and visualizations needed in each part.\n",
    "\n",
    "2. The file you upload must be named as `[Student ID]-[Your name].zip`.\n",
    "\n",
    "3. Your notebook must be executed without any problem. If not, you will lose points for each part consequently. \n",
    "\n",
    "4. **Important Note:** The outputs of the code blocks must be remained in your notebook, otherwise, you definitly lose all the points of that part.\n",
    "\n",
    "\n",
    "In case you have any questions, contact **mohammad99hashemi@gmail.com** or **naghmefarzi@gmail.com**.\n",
    "\n",
    "Good luck :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
