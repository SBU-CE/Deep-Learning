{"cells":[{"cell_type":"markdown","id":"ultimate-orange","metadata":{"id":"ultimate-orange"},"source":["# Sentiment Analysis with LSTMs\n","\n","In this notebook, we'll implement a recurrent neural network that performs sentiment analysis."]},{"cell_type":"markdown","id":"minus-belle","metadata":{"id":"minus-belle"},"source":["## 0. Load in and visualize the data"]},{"cell_type":"code","execution_count":1,"id":"soviet-graphics","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soviet-graphics","executionInfo":{"status":"ok","timestamp":1653199601223,"user_tz":-270,"elapsed":1903,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"3197ad3d-5e5b-4211-ccc8-1575c4c58703"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-22 06:06:39--  https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/labels.txt\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/labels.txt [following]\n","--2022-05-22 06:06:39--  https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/labels.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 225000 (220K) [text/plain]\n","Saving to: ‘labels.txt’\n","\n","labels.txt          100%[===================>] 219.73K  --.-KB/s    in 0.02s   \n","\n","2022-05-22 06:06:40 (8.91 MB/s) - ‘labels.txt’ saved [225000/225000]\n","\n","--2022-05-22 06:06:40--  https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/reviews.txt\n","Resolving github.com (github.com)... 140.82.113.3\n","Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/reviews.txt [following]\n","--2022-05-22 06:06:40--  https://raw.githubusercontent.com/agungsantoso/deep-learning-v2-pytorch/master/sentiment-rnn/data/reviews.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 33678267 (32M) [text/plain]\n","Saving to: ‘reviews.txt’\n","\n","reviews.txt         100%[===================>]  32.12M   190MB/s    in 0.2s    \n","\n","2022-05-22 06:06:40 (190 MB/s) - ‘reviews.txt’ saved [33678267/33678267]\n","\n"]}],"source":["!mkdir data\n","!wget -c https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/labels.txt\n","!wget -c https://github.com/agungsantoso/deep-learning-v2-pytorch/raw/master/sentiment-rnn/data/reviews.txt\n","!mv *.txt data/"]},{"cell_type":"code","execution_count":2,"id":"australian-clark","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"australian-clark","executionInfo":{"status":"ok","timestamp":1653199609597,"user_tz":-270,"elapsed":5,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"330cc358-28c5-4040-83fd-b01f31ddc050"},"outputs":[{"output_type":"stream","name":"stdout","text":["bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n","story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n","\n","positive\n","negative\n","positive\n","neg\n"]}],"source":["# see a sample of the dataset\n","import numpy as np\n","\n","with open(\"data/reviews.txt\", 'r') as f:\n","    reviews = f.read()\n","\n","with open(\"data/labels.txt\", 'r') as f:\n","    labels = f.read()\n","    \n","print(reviews[:1000])\n","print()\n","print(labels[:30])"]},{"cell_type":"markdown","id":"instructional-packaging","metadata":{"id":"instructional-packaging"},"source":["## 1. Data pre-processing\n"]},{"cell_type":"code","execution_count":3,"id":"golden-clark","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"golden-clark","executionInfo":{"status":"ok","timestamp":1653199617410,"user_tz":-270,"elapsed":3878,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"9268741a-bae1-4d10-96fa-0ad708c81ec1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Built-in punctuations: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"]}],"source":["from string import punctuation\n","\n","print(f\"Built-in punctuations: {punctuation}\")\n","\n","# get rid of punctuations\n","reviews = reviews.lower() # lower\n","all_text = ''.join([c for c in reviews if c not in punctuation])"]},{"cell_type":"code","execution_count":4,"id":"distributed-knowing","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"distributed-knowing","executionInfo":{"status":"ok","timestamp":1653199619783,"user_tz":-270,"elapsed":499,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"5445d4f3-cf03-4359-86f5-1cb5cd8c7e8e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bromwell',\n"," 'high',\n"," 'is',\n"," 'a',\n"," 'cartoon',\n"," 'comedy',\n"," 'it',\n"," 'ran',\n"," 'at',\n"," 'the',\n"," 'same',\n"," 'time',\n"," 'as',\n"," 'some',\n"," 'other',\n"," 'programs',\n"," 'about',\n"," 'school',\n"," 'life',\n"," 'such',\n"," 'as',\n"," 'teachers',\n"," 'my',\n"," 'years',\n"," 'in',\n"," 'the',\n"," 'teaching',\n"," 'profession',\n"," 'lead',\n"," 'me']"]},"metadata":{},"execution_count":4}],"source":["# split by new lines and spaces\n","reviews_split = all_text.split('\\n')\n","all_text = ' '.join(reviews_split)\n","\n","# create a list of words\n","words = all_text.split()\n","\n","words[:30]"]},{"cell_type":"markdown","id":"fifty-juvenile","metadata":{"id":"fifty-juvenile"},"source":["### Encoding the words\n"]},{"cell_type":"code","execution_count":5,"id":"earned-borough","metadata":{"id":"earned-borough","executionInfo":{"status":"ok","timestamp":1653199626589,"user_tz":-270,"elapsed":1916,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}}},"outputs":[],"source":["from collections import Counter\n","\n","# build a dictionary that maps words to integers\n","counts = Counter(words)\n","vocab = sorted(counts, key=counts.get, reverse=True)\n","vocab_to_int = {word: i for i, word in enumerate(vocab, 1)} \n","\n","# use the dict to tokenize each review in reviews_split\n","# store the tokenized reviews in reviews_ints\n","reviews_ints = []\n","for review in reviews_split:\n","    reviews_ints.append([vocab_to_int[word] for word in review.split()])"]},{"cell_type":"code","execution_count":6,"id":"requested-argument","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"requested-argument","executionInfo":{"status":"ok","timestamp":1653199633235,"user_tz":-270,"elapsed":495,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"6c4580ea-174c-4819-9108-f18c050e5320"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique words: 74072\n","Tokenized review: \n"," [21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]\n"]}],"source":["# stats about the vocabulary\n","print(\"Unique words:\", len(vocab_to_int))\n","\n","# print tokens in first review\n","print(\"Tokenized review: \\n\", reviews_ints[0])"]},{"cell_type":"markdown","id":"statistical-journal","metadata":{"id":"statistical-journal"},"source":["### Encoding the labels"]},{"cell_type":"code","execution_count":7,"id":"editorial-cross","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"editorial-cross","executionInfo":{"status":"ok","timestamp":1653199637388,"user_tz":-270,"elapsed":712,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"94bce7eb-8ea0-45a8-d332-8df34690bbff"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])"]},"metadata":{},"execution_count":7}],"source":["# 1=positive, 0=negative\n","labels_split = labels.split('\\n')\n","encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])\n","\n","encoded_labels[:10]"]},{"cell_type":"markdown","id":"gothic-softball","metadata":{"id":"gothic-softball"},"source":["### Removing outliers"]},{"cell_type":"code","execution_count":8,"id":"extraordinary-fifty","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"extraordinary-fifty","executionInfo":{"status":"ok","timestamp":1653199638451,"user_tz":-270,"elapsed":4,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"54b96856-05b4-43f3-830f-55e359749b89"},"outputs":[{"output_type":"stream","name":"stdout","text":["Zero-length reviews: 1\n","Maximum-length reviews: 2514\n"]}],"source":["# outlier review stats\n","review_lens = Counter([len(x) for x in reviews_ints])\n","print(f\"Zero-length reviews: {review_lens[0]}\")\n","print(f\"Maximum-length reviews: {max(review_lens)}\")"]},{"cell_type":"code","execution_count":9,"id":"immediate-harvard","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"immediate-harvard","executionInfo":{"status":"ok","timestamp":1653199640934,"user_tz":-270,"elapsed":4,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"f3afdafb-b8d4-4509-afb1-407e59ec80c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of reviews before removing outliers:  25001\n","Number of reviews after removing outliers:  25000\n"]}],"source":["print(\"Number of reviews before removing outliers: \", len(reviews_ints))\n","\n","# remove any reviews/labels with zero length from the reviews_ints list\n","non_zero_idx = [i for i, review in enumerate(reviews_ints) if len(review)]\n","reviews_ints = [reviews_ints[i] for i in non_zero_idx]\n","encoded_labels = np.array([encoded_labels[i] for i in non_zero_idx])\n","\n","print(\"Number of reviews after removing outliers: \", len(reviews_ints))\n"]},{"cell_type":"markdown","id":"express-currency","metadata":{"id":"express-currency"},"source":["### Padding sequences"]},{"cell_type":"code","execution_count":10,"id":"planned-restriction","metadata":{"id":"planned-restriction","executionInfo":{"status":"ok","timestamp":1653199642924,"user_tz":-270,"elapsed":4,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}}},"outputs":[],"source":["def pad_features(reviews_ints, seq_length):\n","    \n","    # getting the correct (rowsxcols) shape\n","    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n","    \n","    # for each review, pad from the left of the sequence\n","    for i, row in enumerate(reviews_ints):\n","        features[i, -len(row):] = np.array(row)[:seq_length]\n","        \n","    return features"]},{"cell_type":"code","execution_count":11,"id":"received-pointer","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"received-pointer","executionInfo":{"status":"ok","timestamp":1653199645102,"user_tz":-270,"elapsed":414,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"2e29fbf4-91bc-41fe-cbca-af8786cd349c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [22382    42 46418    15   706 17139  3389    47    77    35]\n"," [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [   54    10    14   116    60   798   552    71   364     5]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    1   330   578    34     3   162   748  2731     9   325]\n"," [    9    11 10171  5305  1946   689   444    22   280   673]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [   21   122  2069  1565   515  8181    88     6  1325  1182]\n"," [    1    20     6    76    40     6    58    81    95     5]\n"," [   54    10    84   329 26230 46427    63    10    14   614]\n"," [   11    20     6    30  1436 32317  3769   690 15100     6]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [   40    26   109 17952  1422     9     1   327     4   125]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [   10   499     1   307 10399    55    74     8    13    30]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]]\n"]}],"source":["# teset pad_features\n","seq_length = 200\n","features = pad_features(reviews_ints, seq_length)\n","\n","print(features[:30, :10])"]},{"cell_type":"markdown","id":"regulation-twenty","metadata":{"id":"regulation-twenty"},"source":["## 2. Dataloaders and Batching"]},{"cell_type":"code","execution_count":12,"id":"mysterious-village","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mysterious-village","executionInfo":{"status":"ok","timestamp":1653199647603,"user_tz":-270,"elapsed":426,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"c6da2477-30a7-4323-a95f-c5bace7d7f02"},"outputs":[{"output_type":"stream","name":"stdout","text":["shape train_X: (20000, 200)\n","shape val_X: (2500, 200)\n","shape test_X: (2500, 200)\n","shape train_y: (20000,)\n","shape val_y: (2500,)\n","shape test_y: (2500,)\n"]}],"source":["# split data into training, validation, and test data\n","split_frac = 0.8\n","\n","split_idx = int(len(features)*0.8)\n","train_X, remaining_x = features[:split_idx], features[split_idx:]\n","train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n","\n","test_idx = int(len(remaining_x)*0.5)\n","val_X, test_X = remaining_x[:test_idx], remaining_x[test_idx:]\n","val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n","\n","print(\"shape train_X:\", train_X.shape)\n","print(\"shape val_X:\", val_X.shape)\n","print(\"shape test_X:\", test_X.shape)\n","print(\"shape train_y:\", train_y.shape)\n","print(\"shape val_y:\", val_y.shape)\n","print(\"shape test_y:\", test_y.shape)"]},{"cell_type":"code","execution_count":22,"id":"interim-blues","metadata":{"id":"interim-blues","executionInfo":{"status":"ok","timestamp":1653200814031,"user_tz":-270,"elapsed":6,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# create tensor datasets\n","train_data = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(train_y))\n","valid_data = TensorDataset(torch.from_numpy(val_X), torch.from_numpy(val_y))\n","test_data = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(test_y))\n","\n","# create data loaders\n","batch_size = 50\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"]},{"cell_type":"markdown","id":"favorite-tucson","metadata":{"id":"favorite-tucson"},"source":["## 3. Sentiment Network (RNN)\n"]},{"cell_type":"code","execution_count":23,"id":"precise-render","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"precise-render","executionInfo":{"status":"ok","timestamp":1653200818421,"user_tz":-270,"elapsed":7,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"bbf03422-7566-4946-8e56-349790ed0cc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Your program will run on GPU? True\n"]}],"source":["# first check if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","print(\"Your program will run on GPU?\", train_on_gpu)"]},{"cell_type":"markdown","id":"light-colony","metadata":{"id":"light-colony"},"source":["### Model"]},{"cell_type":"code","execution_count":24,"id":"optimum-seminar","metadata":{"id":"optimum-seminar","executionInfo":{"status":"ok","timestamp":1653200818422,"user_tz":-270,"elapsed":3,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class SentimentRNN(nn.Module):\n","    \"\"\"\n","    The RNN model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n","        \"\"\"\n","        Initialize the model by setting up the layers.\n","        \"\"\"\n","        super(SentimentRNN, self).__init__()\n","\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        \n","        # embedding and LSTM layers\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n","                            dropout=drop_prob, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","        \n","        # linear and sigmoid layer\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size = x.size(0)\n","        \n","        # embeddings and lstm_out\n","        embeds = self.embedding(x)\n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","        \n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","        # dropout and fully connected layer\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)\n","        \n","        # sigmoid function\n","        sig_out = self.sig(out)\n","        \n","        # reshape to be batch_size first\n","        sig_out = sig_out.view(batch_size, -1)\n","        sig_out = sig_out[:, -1] # get last batch of labels\n","        \n","        # return last sigmoid output and hidden state\n","        return sig_out, hidden\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if(train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                       weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                        weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden\n","        "]},{"cell_type":"code","execution_count":25,"id":"moderate-fellowship","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"moderate-fellowship","executionInfo":{"status":"ok","timestamp":1653200819538,"user_tz":-270,"elapsed":5,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"2d62e9ed-90cd-4776-9028-79f438975be9"},"outputs":[{"output_type":"stream","name":"stdout","text":["SentimentRNN(\n","  (embedding): Embedding(74073, 400)\n","  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=256, out_features=1, bias=True)\n","  (sig): Sigmoid()\n",")\n"]}],"source":["# instantiate the model\n","vocab_size = len(vocab_to_int) + 1 # +1 for zero padding\n","output_size = 1 # pos/neg\n","embedding_dim = 400\n","hidden_dim = 256 # number of units of a lstm layer\n","n_layers = 2 # number of lstm layers\n","\n","net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","\n","print(net)"]},{"cell_type":"markdown","id":"wooden-plastic","metadata":{"id":"wooden-plastic"},"source":["## 4. Training"]},{"cell_type":"code","execution_count":26,"id":"distinct-fabric","metadata":{"id":"distinct-fabric","executionInfo":{"status":"ok","timestamp":1653200819538,"user_tz":-270,"elapsed":2,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}}},"outputs":[],"source":["# loss and optimization functions\n","lr = 0.001\n","\n","criterion = nn.BCELoss() # binary cross entropy loss\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":29,"id":"trained-first","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trained-first","executionInfo":{"status":"ok","timestamp":1653200953291,"user_tz":-270,"elapsed":88235,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"ca7a94f9-a3f5-4c6b-92e9-d22382dba5f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/4... Step: 100... Loss: 0.640346... Val Loss: 0.610957\n","Epoch: 1/4... Step: 200... Loss: 0.419593... Val Loss: 0.511323\n","Epoch: 1/4... Step: 300... Loss: 0.452594... Val Loss: 0.499771\n","Epoch: 1/4... Step: 400... Loss: 0.277503... Val Loss: 0.453776\n","Epoch: 2/4... Step: 500... Loss: 0.305157... Val Loss: 0.436130\n","Epoch: 2/4... Step: 600... Loss: 0.258760... Val Loss: 0.465048\n","Epoch: 2/4... Step: 700... Loss: 0.424110... Val Loss: 0.443332\n","Epoch: 2/4... Step: 800... Loss: 0.129274... Val Loss: 0.444443\n","Epoch: 3/4... Step: 900... Loss: 0.155316... Val Loss: 0.486795\n","Epoch: 3/4... Step: 1000... Loss: 0.254357... Val Loss: 0.476492\n","Epoch: 3/4... Step: 1100... Loss: 0.175948... Val Loss: 0.532475\n","Epoch: 3/4... Step: 1200... Loss: 0.396188... Val Loss: 0.565588\n","Epoch: 4/4... Step: 1300... Loss: 0.124833... Val Loss: 0.553910\n","Epoch: 4/4... Step: 1400... Loss: 0.156098... Val Loss: 0.564738\n","Epoch: 4/4... Step: 1500... Loss: 0.103322... Val Loss: 0.523830\n","Epoch: 4/4... Step: 1600... Loss: 0.155168... Val Loss: 0.543123\n"]}],"source":["# training params\n","\n","epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 100\n","clip=5 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","for e in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    # batch loop\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            inputs, labels = inputs.cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","\n","        output, h = net(inputs, h)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.float())\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","        # loss stats\n","        if counter % print_every == 0:\n","            # Get validation loss\n","            val_h = net.init_hidden(batch_size)\n","            val_losses = []\n","            net.eval()\n","            for inputs, labels in valid_loader:\n","\n","                # Creating new variables for the hidden state, otherwise\n","                # we'd backprop through the entire training history\n","                val_h = tuple([each.data for each in val_h])\n","\n","                if(train_on_gpu):\n","                    inputs, labels = inputs.cuda(), labels.cuda()\n","\n","                output, val_h = net(inputs, val_h)\n","                val_loss = criterion(output.squeeze(), labels.float())\n","\n","                val_losses.append(val_loss.item())\n","\n","            net.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Loss: {:.6f}...\".format(loss.item()),\n","                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"]},{"cell_type":"markdown","id":"informal-bonus","metadata":{"id":"informal-bonus"},"source":["## 5. Testing"]},{"cell_type":"code","execution_count":30,"id":"invisible-oasis","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"invisible-oasis","executionInfo":{"status":"ok","timestamp":1653200970340,"user_tz":-270,"elapsed":1032,"user":{"displayName":"Mohammad Hashemi","userId":"13551423653125560744"}},"outputId":"fec9ea1a-0671-4b29-d7b0-675fa98311d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss: 0.508\n","Test accuracy: 0.808\n"]}],"source":["# Get test data loss and accuracy\n","\n","test_losses = [] # track loss\n","num_correct = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.eval()\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    h = tuple([each.data for each in h])\n","\n","    if(train_on_gpu):\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","    \n","    # get predicted outputs\n","    output, h = net(inputs, h)\n","    \n","    # calculate loss\n","    test_loss = criterion(output.squeeze(), labels.float())\n","    test_losses.append(test_loss.item())\n","    \n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(labels.float().view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct += np.sum(correct)\n","\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","test_acc = num_correct/len(test_loader.dataset)\n","print(\"Test accuracy: {:.3f}\".format(test_acc))"]},{"cell_type":"code","execution_count":null,"id":"accompanied-specialist","metadata":{"id":"accompanied-specialist"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"name":"sentiment_analysis_using_LSTMs.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}